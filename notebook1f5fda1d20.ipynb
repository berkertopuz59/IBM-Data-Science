{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-14T20:54:43.749431Z","iopub.execute_input":"2022-01-14T20:54:43.749718Z","iopub.status.idle":"2022-01-14T20:54:43.754692Z","shell.execute_reply.started":"2022-01-14T20:54:43.749683Z","shell.execute_reply":"2022-01-14T20:54:43.753738Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2022-01-14T20:54:43.756831Z","iopub.execute_input":"2022-01-14T20:54:43.757229Z","iopub.status.idle":"2022-01-14T20:54:43.764810Z","shell.execute_reply.started":"2022-01-14T20:54:43.757200Z","shell.execute_reply":"2022-01-14T20:54:43.763823Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2022-01-14T20:54:43.766032Z","iopub.execute_input":"2022-01-14T20:54:43.766815Z","iopub.status.idle":"2022-01-14T20:54:43.773256Z","shell.execute_reply.started":"2022-01-14T20:54:43.766774Z","shell.execute_reply":"2022-01-14T20:54:43.772379Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset): #Inherits from torch.utils.data.Dataset\n    \n    def __init__(self):\n        #default directory where data is loaded\n        self.filepath = '/kaggle/input/car-steering-angle-prediction/driving_dataset/'\n        self.filenames = os.listdir(self.filepath)\n        angles = []\n        with open(self.filepath + 'angles.txt') as f:\n                b = []\n                for line in f:\n                    filename , angle = line.split()\n                    angles.append([filename, np.float(angle)])\n                    b.append(np.float(angle))\n        #Mean and std of angle values for normalization:\n        self.avg_angle = np.mean(b)\n        self.std_angle = np.std(b)\n        self.angles = angles\n        \n    def __len__(self):\n        return len(self.angles)\n\n    def __getitem__(self, index):\n        filename, angle = self.angles[index]\n        img = cv2.imread(self.filepath + filename)\n        #Resizing images to (66, 200)\n        resized = cv2.resize(img, (66, 200), interpolation = cv2.INTER_AREA)\n        #return the image converted to a numpy array its corresponding steering angle\n        return torch.from_numpy(resized.transpose()).float(), torch.tensor((angle-self.avg_angle)/self.std_angle, dtype = torch.float).reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T20:54:43.774657Z","iopub.execute_input":"2022-01-14T20:54:43.775273Z","iopub.status.idle":"2022-01-14T20:54:43.789097Z","shell.execute_reply.started":"2022-01-14T20:54:43.775235Z","shell.execute_reply":"2022-01-14T20:54:43.788191Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"all_data = Dataset()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T20:54:43.792033Z","iopub.execute_input":"2022-01-14T20:54:43.792573Z","iopub.status.idle":"2022-01-14T20:54:43.887947Z","shell.execute_reply.started":"2022-01-14T20:54:43.792535Z","shell.execute_reply":"2022-01-14T20:54:43.887106Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"all_data.__len__()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T20:54:43.889199Z","iopub.execute_input":"2022-01-14T20:54:43.889912Z","iopub.status.idle":"2022-01-14T20:54:43.896015Z","shell.execute_reply.started":"2022-01-14T20:54:43.889872Z","shell.execute_reply":"2022-01-14T20:54:43.895218Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"45406"},"metadata":{}}]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(all_data, batch_size = 40, shuffle = False, sampler = list(range(0,25570)), num_workers=2, pin_memory=True)\nval_loader = torch.utils.data.DataLoader(all_data, batch_size = 20, shuffle = False, sampler = list(range(25570,35406)),num_workers=2,pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(all_data, batch_size = 20, shuffle = False, sampler = list(range(35406,45406)), num_workers=2,pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T20:54:43.897372Z","iopub.execute_input":"2022-01-14T20:54:43.898126Z","iopub.status.idle":"2022-01-14T20:54:43.909748Z","shell.execute_reply.started":"2022-01-14T20:54:43.898088Z","shell.execute_reply":"2022-01-14T20:54:43.908829Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print(train_loader.__len__(), val_loader.__len__(), test_loader.__len__())","metadata":{"execution":{"iopub.status.busy":"2022-01-14T20:54:43.910894Z","iopub.execute_input":"2022-01-14T20:54:43.911664Z","iopub.status.idle":"2022-01-14T20:54:43.919755Z","shell.execute_reply.started":"2022-01-14T20:54:43.911629Z","shell.execute_reply":"2022-01-14T20:54:43.918814Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"640 492 500\n","output_type":"stream"}]},{"cell_type":"code","source":"class ConvNet(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        \n        #Convolution Layers:\n        self.conv1 = nn.Conv2d(3, 24, 5, stride=2)\n        self.conv2 = nn.Conv2d(24, 36, 5, stride=2)\n        self.conv3 = nn.Conv2d(36, 48, 5, stride=2)\n        self.conv4 = nn.Conv2d(48, 64, 3)\n        self.conv5 = nn.Conv2d(64, 64, 3)\n        \n        #Fully Connected Layers:\n        self.fc1 = nn.Linear(64 * 18, 100)\n        self.fc2 = nn.Linear(100, 50)\n        self.fc3 = nn.Linear(50, 10)\n        self.out = nn.Linear(10, 1)\n\n    def forward(self, x):\n        \n        x = transforms.Normalize(x.mean(), x.std())(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        \n        x = torch.flatten(x, 1)\n        \n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        x = self.out(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-14T20:54:43.922875Z","iopub.execute_input":"2022-01-14T20:54:43.923272Z","iopub.status.idle":"2022-01-14T20:54:43.935032Z","shell.execute_reply.started":"2022-01-14T20:54:43.923184Z","shell.execute_reply":"2022-01-14T20:54:43.934310Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# defining the model\nnet = ConvNet().to(device)\n\n# defining the optimizer\noptimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n\n# defining the loss function\ncriterion = torch.nn.MSELoss().to(device)\n    \nprint(net)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T20:54:43.936221Z","iopub.execute_input":"2022-01-14T20:54:43.936917Z","iopub.status.idle":"2022-01-14T20:54:43.954320Z","shell.execute_reply.started":"2022-01-14T20:54:43.936865Z","shell.execute_reply":"2022-01-14T20:54:43.953630Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"ConvNet(\n  (conv1): Conv2d(3, 24, kernel_size=(5, 5), stride=(2, 2))\n  (conv2): Conv2d(24, 36, kernel_size=(5, 5), stride=(2, 2))\n  (conv3): Conv2d(36, 48, kernel_size=(5, 5), stride=(2, 2))\n  (conv4): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1))\n  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n  (fc1): Linear(in_features=1152, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=50, bias=True)\n  (fc3): Linear(in_features=50, out_features=10, bias=True)\n  (out): Linear(in_features=10, out_features=1, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(5):\n    \n    #Training step:\n    \n    net.train().to(device)\n    \n    train_loss = 0\n    for data, output in train_loader:\n        \n        # Clear the gradients\n        optimizer.zero_grad()\n        \n        # Forward Pass\n        pred = net(data.to(device))\n        \n        # Find the Loss\n        loss = criterion(pred.to(device),output.to(device))\n        \n        # Calculate gradients \n        loss.backward()\n        \n        # Update Weights\n        optimizer.step()\n        \n        # Calculate Loss\n        train_loss = train_loss + loss.item()\n    \n    #Validation step:\n    \n    net.eval().to(device)\n    \n    val_loss = 0\n    \n    for data, output in val_loader:\n        \n        pred = net(data.to(device))\n        loss = criterion(pred.to(device),output.to(device))\n        val_loss = val_loss + loss.item()\n\n    print(f'Epoch {i+1} - Training Loss: {train_loss / len(train_loader)} \\t Validation Loss: {val_loss / len(val_loader)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-14T20:54:43.955565Z","iopub.execute_input":"2022-01-14T20:54:43.956239Z","iopub.status.idle":"2022-01-14T21:03:19.032993Z","shell.execute_reply.started":"2022-01-14T20:54:43.956205Z","shell.execute_reply":"2022-01-14T21:03:19.032109Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch 1 - Training Loss: 1.066382220253331 \t Validation Loss: 1.1508616501568751\nEpoch 2 - Training Loss: 1.0583330885728173 \t Validation Loss: 1.1438412735738464\nEpoch 3 - Training Loss: 1.0507213294128745 \t Validation Loss: 1.1265163017956739\nEpoch 4 - Training Loss: 1.0376251713540114 \t Validation Loss: 1.1071344910251206\nEpoch 5 - Training Loss: 1.026349757817266 \t Validation Loss: 1.090212330337155\n","output_type":"stream"}]},{"cell_type":"code","source":"net.eval().to(device)\n\ntest_loss = 0\n    \nfor data, output in test_loader:\n    pred = net(data.to(device))\n    loss = criterion(pred.to(device),output.to(device))\n    test_loss = test_loss + loss.item()\n\nprint(f'Test Loss: {test_loss / len(test_loader)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-14T21:03:19.036275Z","iopub.execute_input":"2022-01-14T21:03:19.036584Z","iopub.status.idle":"2022-01-14T21:03:48.378957Z","shell.execute_reply.started":"2022-01-14T21:03:19.036549Z","shell.execute_reply":"2022-01-14T21:03:48.377378Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Test Loss: 0.7164886553969118\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Bonus Exercise - Part 1:**","metadata":{}},{"cell_type":"markdown","source":"Chose learning rate, batch size and l2 regularization term as hyperparameters to be tuned. So, new training method with random search is as follows:","metadata":{}},{"cell_type":"code","source":"def random_search(n_iter, n_epoch):\n    \n    best_val_loss = np.inf\n    \n    for j in range(n_iter):\n    \n        # defining hyperparameters\n\n        batch = np.int(np.random.randint(10,50,1))\n        learning_rate = np.float(np.random.uniform(low=0.0005, high=0.005, size=1))\n        reg_term = np.float(np.random.uniform(low=0.000005, high=0.05, size=1))\n        \n        print(f'Batch Size: {batch} - Learning Rate: {learning_rate} - Regularization Term: {reg_term}')\n\n        train_loader = torch.utils.data.DataLoader(all_data, batch_size = 2*batch, shuffle = False, sampler = list(range(0,25570)), num_workers=2, pin_memory=True)\n        val_loader = torch.utils.data.DataLoader(all_data, batch_size = batch, shuffle = False, sampler = list(range(25570,35406)),num_workers=2,pin_memory=True)\n        test_loader = torch.utils.data.DataLoader(all_data, batch_size = batch, shuffle = False, sampler = list(range(35406,45406)), num_workers=2,pin_memory=True)\n\n        # defining the model\n        net = ConvNet().to(device)\n\n        # defining the optimizer\n        optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate, weight_decay = reg_term)\n\n        # defining the loss function\n        criterion = torch.nn.MSELoss().to(device)\n           \n\n        for i in range(n_epoch):\n    \n            #Training step:\n\n            net.train().to(device)\n\n            train_loss = 0\n            for data, output in train_loader:\n\n                # Clear the gradients\n                optimizer.zero_grad()\n\n                # Forward Pass\n                pred = net(data.to(device))\n\n                # Find the Loss\n                loss = criterion(pred.to(device),output.to(device))\n\n                # Calculate gradients \n                loss.backward()\n\n                # Update Weights\n                optimizer.step()\n\n                # Calculate Loss\n                train_loss = train_loss + loss.item()\n\n            #Validation step:\n\n            net.eval().to(device)\n\n            val_loss = 0\n\n            for data, output in val_loader:\n\n                pred = net(data.to(device))\n                loss = criterion(pred.to(device),output.to(device))\n                val_loss = val_loss + loss.item()\n\n            print(f'Iteration {j+1} - Epoch {i+1} - Training Loss: {train_loss / len(train_loader)} \\t Validation Loss: {val_loss / len(val_loader)}')\n\n        if (np.isnan(val_loss) == False) and (val_loss / len(val_loader) < best_val_loss):\n            best_batch_size = batch\n            best_learning_rate = learning_rate\n            best_reg_term = reg_term\n            best_val_loss = val_loss / len(val_loader)\n    \n    return best_batch_size, best_learning_rate, best_reg_term, best_val_loss\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-14T21:03:48.380408Z","iopub.execute_input":"2022-01-14T21:03:48.381268Z","iopub.status.idle":"2022-01-14T21:03:48.396499Z","shell.execute_reply.started":"2022-01-14T21:03:48.381224Z","shell.execute_reply":"2022-01-14T21:03:48.395841Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"opt_batch_size, opt_learning_rate, opt_reg_term, final_val_loss = random_search(5, 5)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T21:03:48.397614Z","iopub.execute_input":"2022-01-14T21:03:48.398278Z","iopub.status.idle":"2022-01-14T21:46:35.659792Z","shell.execute_reply.started":"2022-01-14T21:03:48.398240Z","shell.execute_reply":"2022-01-14T21:46:35.658225Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Batch Size: 19 - Learning Rate: 0.0042309220429979835 - Regularization Term: 0.0061103124046041945\nIteration 1 - Epoch 1 - Training Loss: 1.0370597895030058 \t Validation Loss: 1.1112174195010527\nIteration 1 - Epoch 2 - Training Loss: 1.0011677599962783 \t Validation Loss: 1.097430596907673\nIteration 1 - Epoch 3 - Training Loss: 1.0305084532128157 \t Validation Loss: 1.1535406459073516\nIteration 1 - Epoch 4 - Training Loss: 1.0587839913925217 \t Validation Loss: 1.1537386206289908\nIteration 1 - Epoch 5 - Training Loss: 1.0590404319499143 \t Validation Loss: 1.1535680291928276\nBatch Size: 33 - Learning Rate: 0.004917055680739997 - Regularization Term: 0.03341269623580725\nIteration 2 - Epoch 1 - Training Loss: 1.052062285079993 \t Validation Loss: 1.1317440553804443\nIteration 2 - Epoch 2 - Training Loss: 1.0501440556451513 \t Validation Loss: 1.1072102683231493\nIteration 2 - Epoch 3 - Training Loss: 1.038136799845721 \t Validation Loss: 1.0927837379167735\nIteration 2 - Epoch 4 - Training Loss: 1.031657016594725 \t Validation Loss: 1.0797960984043273\nIteration 2 - Epoch 5 - Training Loss: 1.0283469145173343 \t Validation Loss: 1.070718438406323\nBatch Size: 21 - Learning Rate: 0.0025298616762345905 - Regularization Term: 0.010866788417482457\nIteration 3 - Epoch 1 - Training Loss: 1.0588954550508543 \t Validation Loss: 1.1375273897515035\nIteration 3 - Epoch 2 - Training Loss: 1.0450549537883418 \t Validation Loss: 1.090351759739938\nIteration 3 - Epoch 3 - Training Loss: 1.0282943280653196 \t Validation Loss: 1.0674679834929754\nIteration 3 - Epoch 4 - Training Loss: 1.0179864177158358 \t Validation Loss: 1.0552789313406663\nIteration 3 - Epoch 5 - Training Loss: 1.0103894821505792 \t Validation Loss: 1.0502943363561825\nBatch Size: 26 - Learning Rate: 0.003080188362156339 - Regularization Term: 0.034281795293730925\nIteration 4 - Epoch 1 - Training Loss: 1.0501618563105184 \t Validation Loss: 1.138891457608889\nIteration 4 - Epoch 2 - Training Loss: 1.0506885315690164 \t Validation Loss: 1.1141739328410127\nIteration 4 - Epoch 3 - Training Loss: 1.0358500311238577 \t Validation Loss: 1.0926514351576722\nIteration 4 - Epoch 4 - Training Loss: 1.026205312705145 \t Validation Loss: 1.076346794998081\nIteration 4 - Epoch 5 - Training Loss: 1.0213752690785138 \t Validation Loss: 1.065802116708085\nBatch Size: 48 - Learning Rate: 0.0044755775809575404 - Regularization Term: 0.019363752584940174\nIteration 5 - Epoch 1 - Training Loss: 1.0587933859332248 \t Validation Loss: 1.1487132768321318\nIteration 5 - Epoch 2 - Training Loss: 1.0584325990914314 \t Validation Loss: 1.140642798564783\nIteration 5 - Epoch 3 - Training Loss: 1.0506737975358758 \t Validation Loss: 1.1170768765553765\nIteration 5 - Epoch 4 - Training Loss: 1.0398126393802942 \t Validation Loss: 1.0979331079705776\nIteration 5 - Epoch 5 - Training Loss: 1.032647295031358 \t Validation Loss: 1.0801573548255852\n","output_type":"stream"}]},{"cell_type":"code","source":"print(opt_batch_size, opt_learning_rate, opt_reg_term, final_val_loss)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T21:46:35.661786Z","iopub.execute_input":"2022-01-14T21:46:35.662095Z","iopub.status.idle":"2022-01-14T21:46:35.670028Z","shell.execute_reply.started":"2022-01-14T21:46:35.662033Z","shell.execute_reply":"2022-01-14T21:46:35.669357Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"21 0.0025298616762345905 0.010866788417482457 1.0502943363561825\n","output_type":"stream"}]},{"cell_type":"code","source":"# Retraining the model with the best found hyperparameters\n\ntrain_loader = torch.utils.data.DataLoader(all_data, batch_size = 2*opt_batch_size, shuffle = False, sampler = list(range(0,35406)), num_workers=2, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(all_data, batch_size = opt_batch_size, shuffle = False, sampler = list(range(35406,45406)), num_workers=2,pin_memory=True)\n\n# defining the model\nnet = ConvNet().to(device)\n\n# defining the optimizer\noptimizer = torch.optim.SGD(net.parameters(), lr = opt_learning_rate, weight_decay = opt_reg_term)\n\n# defining the loss function\ncriterion = torch.nn.MSELoss().to(device)\n\n\nfor i in range(5):\n\n    #Training step:\n\n    net.train().to(device)\n\n    train_loss = 0\n    for data, output in train_loader:\n\n        # Clear the gradients\n        optimizer.zero_grad()\n\n        # Forward Pass\n        pred = net(data.to(device))\n\n        # Find the Loss\n        loss = criterion(pred.to(device),output.to(device))\n\n        # Calculate gradients \n        loss.backward()\n\n        # Update Weights\n        optimizer.step()\n\n        # Calculate Loss\n        train_loss = train_loss + loss.item()\n\n    print(f'Epoch {i+1} - Training Loss: {train_loss / len(train_loader)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-14T21:46:35.671474Z","iopub.execute_input":"2022-01-14T21:46:35.671754Z","iopub.status.idle":"2022-01-14T21:55:11.773177Z","shell.execute_reply.started":"2022-01-14T21:46:35.671714Z","shell.execute_reply":"2022-01-14T21:55:11.772296Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 1 - Training Loss: 1.0571911428814627\nEpoch 2 - Training Loss: 1.0083178714058452\nEpoch 3 - Training Loss: 0.9876358370082918\nEpoch 4 - Training Loss: 0.9771924934206748\nEpoch 5 - Training Loss: 0.9694529743965252\n","output_type":"stream"}]},{"cell_type":"code","source":"# Testing the final model\n\nnet.eval().to(device)\n\ntest_loss = 0\n    \nfor data, output in test_loader:\n    pred = net(data.to(device))\n    loss = criterion(pred.to(device),output.to(device))\n    test_loss = test_loss + loss.item()\n\nprint(f'Test Loss: {test_loss / len(test_loader)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-14T21:55:11.775736Z","iopub.execute_input":"2022-01-14T21:55:11.776505Z","iopub.status.idle":"2022-01-14T21:55:42.322948Z","shell.execute_reply.started":"2022-01-14T21:55:11.776462Z","shell.execute_reply":"2022-01-14T21:55:42.322112Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Test Loss: 0.7135797410300251\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Bonus Exercise - Part 2:**","metadata":{}},{"cell_type":"code","source":"# defining batches\ntrain_loader = torch.utils.data.DataLoader(all_data, batch_size = 40, shuffle = False, sampler = list(range(0,25570)), num_workers=2, pin_memory=True)\nval_loader = torch.utils.data.DataLoader(all_data, batch_size = 20, shuffle = False, sampler = list(range(25570,35406)),num_workers=2,pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(all_data, batch_size = 20, shuffle = False, sampler = list(range(35406,45406)), num_workers=2,pin_memory=True)\n\n# defining the model\nnet = ConvNet().to(device)\n\n# defining the optimizer\noptimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n\n# defining the loss function\ncriterion = torch.nn.MSELoss().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T21:55:42.325389Z","iopub.execute_input":"2022-01-14T21:55:42.325637Z","iopub.status.idle":"2022-01-14T21:55:42.342986Z","shell.execute_reply.started":"2022-01-14T21:55:42.325608Z","shell.execute_reply":"2022-01-14T21:55:42.342256Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    \n    #Training step:\n    \n    net.train().to(device)\n    \n    train_loss = 0\n    \n    for old_data, output in train_loader:\n        \n        # 5x5 square cutout in the train data\n        data = old_data.clone()\n        index1 = np.int(np.random.randint(0,66,1))\n        index2 = np.int(np.random.randint(0,200,1))\n        data[:, :, max(index1-2,0) : min(index1+3,66), max(index2-2,0) : min(index2+3,200)] = torch.zeros(min(old_data.shape[0],40), 3, min(5,index1+3,(65-index1)+3), min(5,index2+3,(199-index2)+3))\n        \n        # Clear the gradients\n        optimizer.zero_grad()\n        \n        # Forward Pass\n        pred = net(data.to(device))\n        \n        # Find the Loss\n        loss = criterion(pred.to(device),output.to(device))\n        \n        # Calculate gradients \n        loss.backward()\n        \n        # Update Weights\n        optimizer.step()\n        \n        # Calculate Loss\n        train_loss = train_loss + loss.item()\n    \n    #Validation step:\n    \n    net.eval().to(device)\n    \n    val_loss = 0\n    \n    for data, output in val_loader:\n\n        pred = net(data.to(device))\n        loss = criterion(pred.to(device),output.to(device))\n        val_loss = val_loss + loss.item()\n\n    print(f'Epoch {i+1} - Training Loss: {train_loss / len(train_loader)} \\t Validation Loss: {val_loss / len(val_loader)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-14T21:55:42.344493Z","iopub.execute_input":"2022-01-14T21:55:42.344960Z","iopub.status.idle":"2022-01-14T22:04:27.664871Z","shell.execute_reply.started":"2022-01-14T21:55:42.344921Z","shell.execute_reply":"2022-01-14T22:04:27.663998Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Epoch 1 - Training Loss: 1.053992056946504 \t Validation Loss: 1.1421603677653462\nEpoch 2 - Training Loss: 1.0495558118153667 \t Validation Loss: 1.124252471767974\nEpoch 3 - Training Loss: 1.0357076325575576 \t Validation Loss: 1.1064435720897852\nEpoch 4 - Training Loss: 1.0227593964508828 \t Validation Loss: 1.0890764149909098\nEpoch 5 - Training Loss: 1.0151835768234378 \t Validation Loss: 1.068841765852166\n","output_type":"stream"}]},{"cell_type":"code","source":"# Testing the final model\n\nnet.eval().to(device)\n\ntest_loss = 0\n    \nfor data, output in test_loader:\n    pred = net(data.to(device))\n    loss = criterion(pred.to(device),output.to(device))\n    test_loss = test_loss + loss.item()\n\nprint(f'Test Loss: {test_loss / len(test_loader)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-14T22:04:27.668722Z","iopub.execute_input":"2022-01-14T22:04:27.669327Z","iopub.status.idle":"2022-01-14T22:04:57.221035Z","shell.execute_reply.started":"2022-01-14T22:04:27.669293Z","shell.execute_reply":"2022-01-14T22:04:57.220175Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Test Loss: 0.7063281774220741\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Bonus Exercise - Part 3:**","metadata":{}},{"cell_type":"code","source":"# defining the model\nnet = ConvNet().to(device)\n\n# defining the optimizer\noptimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n\n# defining the loss function\ncriterion = torch.nn.MSELoss().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T22:04:57.222398Z","iopub.execute_input":"2022-01-14T22:04:57.222945Z","iopub.status.idle":"2022-01-14T22:04:57.235821Z","shell.execute_reply.started":"2022-01-14T22:04:57.222898Z","shell.execute_reply":"2022-01-14T22:04:57.234951Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# defining batches\ntrain_loader1 = torch.utils.data.DataLoader(all_data, batch_size = 40, shuffle = False, sampler = list(range(0,12785)), num_workers=2, pin_memory=True)\ntrain_loader2 = torch.utils.data.DataLoader(all_data, batch_size = 40, shuffle = False, sampler = list(range(12785,25570)), num_workers=2, pin_memory=True)\nval_loader = torch.utils.data.DataLoader(all_data, batch_size = 20, shuffle = False, sampler = list(range(25570,35406)),num_workers=2,pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(all_data, batch_size = 20, shuffle = False, sampler = list(range(35406,45406)), num_workers=2,pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T22:04:57.237418Z","iopub.execute_input":"2022-01-14T22:04:57.237842Z","iopub.status.idle":"2022-01-14T22:04:57.248501Z","shell.execute_reply.started":"2022-01-14T22:04:57.237805Z","shell.execute_reply":"2022-01-14T22:04:57.247655Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def cnn_with_mixup(n_epoch, alpha):\n    \n    for i in range(n_epoch):\n    \n        #Training step:\n\n        net.train().to(device)\n\n        train_loss = 0\n\n        for (x1, y1), (x2, y2) in zip(train_loader1, train_loader2):\n\n            lam = np.random.beta(alpha, alpha)\n            data = lam * x1 + (1. - lam) * x2\n            output = lam * y1 + (1. - lam) * y2\n\n            # Clear the gradients\n            optimizer.zero_grad()\n\n            # Forward Pass\n            pred = net(data.to(device))\n\n            # Find the Loss\n            loss = criterion(pred.to(device),output.to(device))\n\n            # Calculate gradients \n            loss.backward()\n\n            # Update Weights\n            optimizer.step()\n\n            # Calculate Loss\n            train_loss = train_loss + loss.item()\n\n        #Validation step:\n\n        net.eval().to(device)\n\n        val_loss = 0\n\n        for data, output in val_loader:\n\n            pred = net(data.to(device))\n            loss = criterion(pred.to(device),output.to(device))\n            val_loss = val_loss + loss.item()\n\n        print(f'Epoch {i+1} - Training Loss: {train_loss / len(train_loader1)} \\t Validation Loss: {val_loss / len(val_loader)}')\n        \n    return net, train_loss / len(train_loader1), val_loss / len(val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T22:04:57.249999Z","iopub.execute_input":"2022-01-14T22:04:57.250526Z","iopub.status.idle":"2022-01-14T22:04:57.262354Z","shell.execute_reply.started":"2022-01-14T22:04:57.250489Z","shell.execute_reply":"2022-01-14T22:04:57.261552Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"net1, final_train_loss, final_val_loss = cnn_with_mixup(5, 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T22:04:57.263826Z","iopub.execute_input":"2022-01-14T22:04:57.264094Z","iopub.status.idle":"2022-01-14T22:13:19.231700Z","shell.execute_reply.started":"2022-01-14T22:04:57.264060Z","shell.execute_reply":"2022-01-14T22:13:19.229943Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Epoch 1 - Training Loss: 1.2089003334803237 \t Validation Loss: 1.1546923892642738\nEpoch 2 - Training Loss: 0.6386410997775783 \t Validation Loss: 1.1505882738166973\nEpoch 3 - Training Loss: 0.7138454163559913 \t Validation Loss: 1.1484468858746597\nEpoch 4 - Training Loss: 1.0064464310776657 \t Validation Loss: 1.148122296361047\nEpoch 5 - Training Loss: 0.7765527388572877 \t Validation Loss: 1.144294100031698\n","output_type":"stream"}]},{"cell_type":"code","source":"net2, final_train_loss, final_val_loss = cnn_with_mixup(5, 0.4)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T22:13:19.233710Z","iopub.execute_input":"2022-01-14T22:13:19.234004Z","iopub.status.idle":"2022-01-14T22:21:38.116392Z","shell.execute_reply.started":"2022-01-14T22:13:19.233963Z","shell.execute_reply":"2022-01-14T22:21:38.115529Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Epoch 1 - Training Loss: 0.5392096665537792 \t Validation Loss: 1.1396616287792112\nEpoch 2 - Training Loss: 0.7760286033865669 \t Validation Loss: 1.1357071537585197\nEpoch 3 - Training Loss: 0.6529489944172099 \t Validation Loss: 1.1271209725482476\nEpoch 4 - Training Loss: 0.5602694084223344 \t Validation Loss: 1.1213893235166679\nEpoch 5 - Training Loss: 0.9030237046434195 \t Validation Loss: 1.0945210009631705\n","output_type":"stream"}]},{"cell_type":"code","source":"# Testing the final model with the second mixup model\n\nnet2.eval().to(device)\n\ntest_loss = 0\n    \nfor data, output in test_loader:\n    pred = net2(data.to(device))\n    loss = criterion(pred.to(device),output.to(device))\n    test_loss = test_loss + loss.item()\n\nprint(f'Test Loss: {test_loss / len(test_loader)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-14T22:21:38.118218Z","iopub.execute_input":"2022-01-14T22:21:38.118763Z","iopub.status.idle":"2022-01-14T22:22:07.687125Z","shell.execute_reply.started":"2022-01-14T22:21:38.118720Z","shell.execute_reply":"2022-01-14T22:22:07.685578Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Test Loss: 0.6949187648328813\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Random search gave the lowest validation error while mixup method gave the fastest result and the lowest test error. Mixup method and cutout method could give better results with better hyperparameter preferences.","metadata":{}}]}